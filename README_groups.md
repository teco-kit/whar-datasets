Got it. That's a very practical distinction. Physiological signals (like ECG, EMG, heart rate) and environmental time-series data (like stretch sensors or audio/MFCC) are fundamentally still 1D time-series data streams, much like IMU data. Reserving Group 3 strictly for non-time-series, high-dimensional modalities (like RGB video, depth cameras, or optical motion capture) makes the boundary much clearer for a machine learning benchmark.

Here are the newly adjusted tables. Group 1 and Group 4 remain exactly as they were before, while Group 2 has been expanded to include all the time-series physiological/ambient datasets. All tables are sorted by citations in descending order, and no cell contents have been changed.

# Other Sources

- https://zenodo.org/records/3831958
- https://zenodo.org/records/13987073 
- http://har-dataset.org/doku.php?id=wiki:dataset
- https://github.com/jindongwang/activityrecognition/blob/master/notes/dataset%20description.md
- https://github.com/haoranD/Awesome-Human-Activity-Recognition

### Group 1: Single Consumer Device & Practical Wearables

*(Single smartphone, smartwatch, or single activity tracker)*

| Supported | Name | Year | Paper | Citations | Subjects | Activities | Sensors |
| --- | --- | --- | --- | --- | --- | --- | --- |
| ✅ | [WISDM-10](https://www.cis.fordham.edu/wisdm/dataset.php) | 2010 | *Activity Recognition using Cell Phone Accelerometers* | 3862 | 36 | 6 (Walking, Jogging, Upstairs, Downstairs, Sitting, Standing) | - |
| ✅ | [UCI-HAR](https://archive.ics.uci.edu/dataset/240/human+activity+recognition+using+smartphones) | 2013 | *A Public Domain Dataset for Human Activity Recognition using Smartphones* | 3372 | 30 (19-48 years) | 6 (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) | smartphone (Samsung Galaxy S II) on the waist, embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz |
| ⬜ | [HAPT](https://archive.ics.uci.edu/dataset/341/smartphone+based+recognition+of+human+activities+and+postural+transitions) | 2016 | *Transition-aware human activity recognition using smartphones.* | 939 | 30 (19-48 years) | 6 (three static postures (standing, sitting, lying) and three dynamic activities (walking, walking downstairs and walking upstairs)) + postural transitions that occurred between the static postures. These are: stand-to-sit, sit-to-stand, sit-to-lie, lie-to-sit, stand-to-lie, and lie-to-stand | wearing a smartphone (Samsung Galaxy S II) on the waist during the experiment execution. We captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz using the embedded accelerometer and gyroscope of the device |
| ✅ | [USC-HAD](https://sipi.usc.edu/had/) | 2012 | *USC-HAD: A Daily Activity Dataset for Ubiquitous Activity Recognition Using Wearable Sensors* | 753 | 14 (age 21 - 49) | 12 activities: walking forward, walking left, walking right, walking upstairs, walking downstairs, running forward, jumping, sitting, standing, sleeping, elevator up, elevator down. | MotionNode, a 6-DOF IMU for human motion sensing, integrates 3-axis accelerometer, gyroscope, and magnetometer, samples up to 100 Hz, and is attached to the front right hip. |
| ⬜ | [UniMiB-SHAR](http://www.sal.disco.unimib.it/technologies/unimib-shar/) | 2017 | *Unimib shar: a dataset for human activity recognition using acceleration data from smartphones* | 712 | 30 subjects, ages 18–60 | 17 (9 ADLs, and 8 types of falls) | Android smartphone accelerometer |
| ✅ | [MotionSense](https://github.com/mmalekzadeh/motion-sense) | 2019 | *Mobile Sensor Data Anonymization* | 345 | 24 participants, ages 18–46 years | 6 activities, walking downstairs (dws), walking upstairs (ups), walking (wlk), jogging (jog), sitting (sit), standing (std), trials mapped to activities | iPhone 6s, phone in front pocket, 50 Hz sampling, sensor types: Accelerometer, Gyroscope, Attitude sensor (from Core Motion), Gravity sensor (software-derived from accelerometer + geomagnetic field), Rotation Rate (gyroscope-based), User Acceleration (accelerometer-based) |
| ⬜ | [RealLifeHAR](https://lbd.udc.es/research/real-life-HAR-dataset/) | 2020 | *A Public Domain Dataset for Real-Life Human Activity Recognition Using Smartphone Sensors* | 208 | 19 | 4 activities — inactive (device not carried), active (moving but not traveling), walking (moving to a specific place, includes running/jogging), driving (using engine-powered transport, e.g., car, bus, motorbike, truck) | smartphone sensors — tri-axial accelerometer, tri-axial gyroscope, tri-axial magnetometer, GPS (latitude, longitude, altitude, bearing, speed, accuracy), variable sampling frequency, orientation- and placement-independent, some participants may lack certain sensors. |
| ✅ | [WISDM-19](https://archive.ics.uci.edu/dataset/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset) | 2019 | *WISDM: Smartphone and Smartwatch Activity and Biometrics Dataset* | 198 | 51 | 18 | smartphone and smartwatch sensors — accelerometer (x, y, z), gyroscope (x, y, z), 20 Hz sampling rate |
| ✅ | [KU-HAR](https://data.mendeley.com/datasets/45f952y38r/5) | 2021 | *KU-HAR: An open dataset for heterogeneous human activity recognition* | 187 | 90 participants (75 male, 15 female) | 18 activities — Stand, Sit, Talk-sit, Talk-stand, Stand-sit, Lay, Lay-stand, Pick, Jump, Push-up, Sit-up, Walk, Walk-backward, Walk-circle, Run, Stair-up, Stair-down, Table-tennis, | smartphone accelerometer (AccX, AccY, AccZ) and gyroscope (GyrX, GyrY, GyrZ), raw and interpolated data, constant 100 Hz sampling |
| ⬜ | [HASC2010](http://hasc.jp/) | 2011 | *Hasc challenge: gathering large scale human activity corpus for the real-world activity understandings* | 157 | 540 subjects total, 96 with full datase | 7 main activities — stay, walk, jog, skip, stair up (stUp), stair down (stDown), sequence | iPhone, iPod Touch, WAA-series (ATR), accelerometer (X, Y, Z axes), 10–100 Hz |
| ⬜ | [Hang-Time](https://ahoelzemann.github.io/hangtime_har/) | 2023 | *Hang-time HAR: A benchmark dataset for basketball activity recognition using wrist-worn inertial sensors* | 52 | 24 | Multi-tier labels include coarse (sessions like warm-up/drills/game), locomotion, basketball, and in/out (court status) | Bangle.js smartwatches recording 3D acceleration at 50 Hz (±8g range), resampled for equidistant timestamps.  |
| ⬜ | [CAPTURE-24](https://ora.ox.ac.uk/objects/uuid:99d7c092-d865-4a19-b096-cc16440cd001) | 2024 | *CAPTURE-24: A large dataset of wrist-worn activity tracker data collected in the wild for human activity recognition* | 45 | 151 | Daily living activities with ground truth including sitting (watching TV), walking (e.g., dog walking), washing dishes, sleeping, and other daily tasks (more than 2,500 hours of labelled activities) | Wrist-worn Axivity AX3 activity tracker (accelerometer-based) |
| ✅ | [HARSense](https://ieee-dataport.org/open-access/harsense-statistical-human-activity-recognition-dataset) | 2021 | *Harsense: statistical human activity recognition dataset* | 5 | 12 | Walking, Standing, Upstairs, Downstairs, Running, Sitting | Smartphone accelerometer, Smartphone gyroscope (waist, front pockets) |

---

### Group 2: Multi-Sensor Networks, Full-Body & Time-Series Signals

*(Expanded: Multiple IMUs/phones, plus any additional 1D time-series modalities like ECG, EMG, heart rate monitors, stretch sensors, or audio).*

| Supported | Name | Year | Paper | Citations | Subjects | Activities | Sensors |
| --- | --- | --- | --- | --- | --- | --- | --- |
| ✅ | [PAMAP2](https://archive.ics.uci.edu/dataset/231/pamap2+physical+activity+monitoring) | 2012 | *Introducing a New Benchmarked Dataset for Activity Monitoring* | 1758 | 9 | 12 (walking, cycling, playing soccer, etc.), 6 optional activities | 3 Colibri wireless inertial measurement units (IMUs), sampling at 100 Hz, positioned on the dominant wrist, the chest, and the dominant ankle; HR monitor sampling at ~9 Hz |
| ✅ | [OPPORTUNITY](https://archive.ics.uci.edu/dataset/226/opportunity+activity+recognition) | 2010 | *Collecting complex activity datasets in highly rich networked sensor environments* | 1024 | 4 (12?) | Annotations are organized on five tracks and multiple levels: modes of locomotion classes (e.g., sitting, standing, walking); low-level actions relating 13 actions to 23 objects (e.g., reach, grasp, release; milk, switch, door); 17 mid-level gesture classes; and 5 high-level activity classes (e.g., preparing a sandwich), grouped into five ADL situations (relaxing, early morning, coffee time, sandwich time, cleanup) | Body-worn sensors: 7 IMUs, 12 3D accelerometers, 4 3D localization sensors; object sensors: 12 objects with 3D acceleration and 2D rate of turn; ambient sensors: 13 switches, 8 3D accelerometers. |
| ⬜ | [HHAR](https://archive.ics.uci.edu/dataset/344/heterogeneity+activity+recognition) | 2015 | *Smart Devices are Different: Assessing and Mitigating Mobile Sensing Heterogeneities for Activity Recognition* | 1019 | 9 | 6 (‘Biking’, ‘Sitting’, ‘Standing’, ‘Walking’, ‘Stair Up’ and ‘Stair down’) | Each user wore two embedded sensors (accelerometer and gyroscope) sampled at the highest device frequency, using different devices, including 4 smartwatches (2 LG, 2 Samsung Galaxy Gear) and 8 smartphones (2 Samsung Galaxy S3 mini, 2 Samsung Galaxy S3, 2 LG Nexus 4, 2 Samsung Galaxy S+) |
| ✅ | [MHEALTH](https://archive.ics.uci.edu/dataset/319/mhealth+dataset) | 2014 | *mHealthDroid: A Novel Framework for Agile Development of Mobile Health Applications* | 887 | 10 | 12 (standing still, sitting and relaxing, lying down, walking, climbing stairs, waist bends forward, frontal arm elevation, knee bends, cycling, jogging, running, jumping front & back.) | 3 sensors attached with elastic straps on the chest, right wrist, and left ankle measure acceleration, rate of turn, and magnetic field orientation to capture body dynamics, with the chest sensor also providing 2-lead ECG (collected for future use, e.g., heart monitoring, arrhythmia detection, or exercise effects), all sampled at 50 Hz. |
| ✅ | [DSADS](https://archive.ics.uci.edu/dataset/256/daily+and+sports+activities) | 2010 | *Comparative study on classifying human activities with miniature inertial and magnetic sensors* | 780 | 8 (4 female, 4 male, between the ages 20 and 30) | 19 (sitting, standing, lying on back, lying on right side, ascending stairs, descending stairs, standing in elevator, moving in elevator, walking in parking lot, walking on treadmill (flat), walking on treadmill (inclined), running on treadmill, stepper exercise, cross trainer exercise, cycling (horizontal), cycling (vertical), rowing, jumping, playing basketball) | Five Xsens MTx units placed on torso, right/left arms, and right/left legs, each with 9 sensors (3-axis accelerometer, gyroscope, magnetometer), calibrated to 25 Hz. |
| ⬜ | [SAD](https://www.utwente.nl/en/eemcs/ps/research/dataset/) | 2014 | *Fusion of Smartphone Motion Sensors for Physical Activity Recognition* | 752 | 10 (male ages 25-30) | 7 (walking, sitting, standing, jogging, biking, walking upstairs and walking downstairs) | Each participant wore five Samsung Galaxy SII smartphones on right/left jean pockets, belt (right leg), right upper arm, and right wrist, recording accelerometer, gyroscope, magnetometer, and linear acceleration data at 50 Hz simultaneously. |
| ✅ | [Daphnet](https://archive.ics.uci.edu/dataset/245/daphnet+freezing+of+gait) | 2009 | *Ambulatory monitoring of freezing of gait in Parkinson’s disease* | 652 | - | 2? | 3 wearable accelerometers placed on the hip and legs. |
| ⬜ |[SisFall](https://www.kaggle.com/datasets/nvnikhil0001/sisfall-enhanced) | 2017 | 584 | *SisFall: A fall and movement dataset* | 38 participants: 23 young adults (19-30 years) and 15 elderly (60-75 years) | 19 ADLs (e.g., walking, sitting, bending) and 15 fall types (e.g., forward/backward slips, trips) | ADXL345 accelerometer (±16g), MMA8451Q accelerometer, ITG3200 gyroscope, sampled at 200 Hz |
| ⬜ | [DIP](https://dip.is.tuebingen.mpg.de/) | 2018 | *Deep inertial poser: Learning to reconstruct human pose from sparse inertial measurements in real time* | 495 | 10 | - | 17 IMUs worn on the body; data includes orientation and motion information |
| ⬜ | [RealWorld](https://www.uni-mannheim.de/dws/research/projects/activity-recognition/dataset/dataset-realworld/) | 2016 | *On-body Localization of Wearable Devices: An Investigation of Position-Aware Activity Recognition* | 482 | 15 subjects (8 males, 7 females, age 31.9 ± 12.4). | 8 activities—walking, running/jogging, sitting, standing, lying, climbing stairs up, climbing stairs down, jumping. | 6 sensors recording acceleration, GPS, gyroscope, light, magnetic field, and sound level on 7 body positions (chest, forearm, head, shin, thigh, upper arm, waist) |
| ⬜ | [ExtraSensory](http://extrasensory.ucsd.edu/) | 2016 | *Recognizing Detailed Human Context In-the-Wild from Smartphones and Smartwatches* | 402 | 60 users (34 female, 26 male, mostly students and research assistants, age 18–42, various ethnicities, handedness, and body stats). | Multi-label behavioral context recognition, including 7 main activities (lying down, sitting, standing in place, standing and moving, walking, running, bicycling) and 109 secondary activities (e.g., sports, transportation, basic needs, company, location), with an average of ~3.8 labels per example; total of 103 cleaned labels applied. | Smartphone and smartwatch sensors, including accelerometer, gyroscope, magnetometer, watch accelerometer, watch compass, audio (MFCC), location, phone state, gravity, and additional sensors (light, air pressure, humidity, temperature), sampled at various frequencies (25–40 Hz for motion sensors, 22 kHz audio, once-per-minute for low-frequency sensors). |
| ⬜ | [UMAFall](https://figshare.com/articles/dataset/UMA_ADL_FALL_Dataset_zip/4214283) | 2017 | *Umafall: A multisensor dataset for the research on automatic fall detection* | 243 | 17 | wide set of ADLs (Activities of Daily Life) and three types of falls | 5 wearable sensing points, which were located on five different points of the body, acceleration, gyroscope and magnetometer data captured simultaneously by four Bluetooth-enabled sensor motes as well as the signals sampled by the accelerometer embedded in a smartphone |
| ⬜ | [REALDISP](https://archive.ics.uci.edu/dataset/305/realdisp+activity+recognition+dataset) | 2014 | *Dealing with the Effects of Sensor Displacement in Wearable Activity Recognition* | 216 | 17 | 33 physical activities including walking, jogging, running, various jumps, trunk and waist movements, arm and shoulder exercises, cycling, rowing, elliptical bike, warm-up and cool-down exercises | 9 body-worn sensors (left calf, left thigh, right calf, right thigh, back, left lower arm, left upper arm, right lower arm, right upper arm), each providing 3D accelerometer (AccX, AccY, AccZ), 3D gyroscope (GyrX, GyrY, GyrZ), 3D magnetometer (MagX, MagY, MagZ), and 4D quaternions (Q1–Q4) |
| ✅ | [HuGaDB](https://github.com/romanchereshnev/HuGaDB) | 2018 | *HuGaDB: Human Gait Database for Activity Recognition from Wearable Inertial Sensor Networks* | 154 | 18 participants (14 male, 4 female, average age 23.67 y) | 12 activities – Walking, Running, Going up stairs, Going down stairs, Sitting, Sitting down, Standing up, Standing, Bicycling, Elevator up, Elevator down, Sitting in car | 6 wearable inertial sensors (accelerometer + gyroscope) on right/left thighs, shins, feet, 2 EMG sensors on quadriceps |
| ⬜ | [HARTH](https://archive.ics.uci.edu/dataset/779/harth) | 2021 | *HARTH: A Human Activity Recognition Dataset for Machine Learning* | 132 | 22 | 12 (walking, running, shuffling, stairs up, stairs down, standing, sitting, lying, cycling (sit), cycling (stand), cycling (sit, inactive), cycling (stand, inactive)) | 2 × 3-axis accelerometers (attached to right thigh and lower back), sampling rate 50 Hz |
| ⬜ | [FallAllD](https://www.kaggle.com/datasets/harnoor343/fall-detection-accelerometer-data) | 2020 | *FallAllD: An Open Dataset of Human Falls and Activities of Daily Living for Classical and Deep Learning Applications* | 119 | 15 subjects (8 males and 7 females) | 44. Twelve of them are related to activities practiced by hand e.g. clapping hands | three data-loggers worn on the waist, wrist and neck, accelerometer, gyroscope, magnetometer and baromete |
| ✅ | [w-HAR](https://github.com/gmbhat/human-activity-recognition) | 2020 | *w-HAR: An Activity Recognition Dataset and Framework Using Low-Power Wearable Devices* | 100 | 22 | 7  | inertial and wearable stretch sensors, thus providing two modalities of activity information |
| ⬜ | [HAR70+](https://archive.ics.uci.edu/dataset/780/har70) | 2021 | *A machine learning classifier for detection of physical activity types and postures during free-living* | 55 | 18 older adults (age 70–95 years) | 7 (walking, shuffling, stairs (ascending), stairs (descending), standing, sitting, lying) | 2 × 3-axial accelerometers (Axivity AX3) — one on the right thigh, one on the lower back, sampling at 50 Hz |
| ⬜ | [TNDA-HAR](https://ieee-dataport.org/open-access/tnda-har-0) | 2022 | *Deep transfer learning with graph neural network for sensor-based human activity recognition* | 48 | 50 | 7 (sitting, standing, and laying, as well as walking, running, cycling, and walking upstairs/downstairs) | 3 wearable locations with tri-axial sensors — wrist, ankle, and back; each sensor includes accelerometer (Acc), gyroscope (Gyr), and magnetometer (Mag) axes (X, Y, Z) |
| ⬜ | [GOTOV](https://data.4tu.nl/articles/dataset/GOTOV_Human_Physical_Activity_and_Energy_Expenditure_Dataset_on_Older_Individuals/12716081) | 2022 | *A recurrent neural network architecture to model physical activity energy expenditure in older people* | 33 | 35 healthy older adults (14 female, 21 male, all over 60 years old) | 16 activities of daily living including low intensity (lying down, sitting), mid intensity (standing, household activities), high intensity (walking, cycling), both indoor and outdoor; some activities recorded at multiple granularities | 3× GeneActiv accelerometers (ankle, wrist, chest), 1× Equivital (chest), COSMED (mask and belt on chest) |
| ⬜ |[RecGym](https://archive.ics.uci.edu/dataset/1128/recgym+gym+workouts+recognition+dataset+with+imu+and+capacitive+sensor-7) | 2022 | *The Contribution of Human Body Capacitance/Body-Area Electric Field To Individual and Collaborative Activity Recognition* | 12 | 10 | 12, including eleven workouts: Adductor, ArmCurl, BenchPress, LegCurl, LegPress, Riding, RopeSkipping, Running, Squat, StairsClimber, Walking, and a "Null" activity when the volunteer hangs around between different workouts session | sensing unit composed of an IMU sensor (columns of A_x, A_y, A_z, G_x, G_y, G_z) and a Body Capacitance sensor (column of C_1). The sensing units were worn at three positions: on the wrist, in the pocket, and on the calf, with a sampling rate of 20 Hz |
| ⬜ | [iSPL](https://github.com/thunguyenth/HAR_IMU_Stretch) | 2022 | *An Investigation on Deep Learning-Based Activity Recognition Using IMUs and Stretch Sensors* | 11 | 1 | 9 (walking, standing, sitting, lying, running, jumping, sit-up, push-up, dancing) | 3 IMUs (right wrist, waist, ankle), 2 fabric stretch sensors (knees) |
| ⬜ | [AReM](https://archive.ics.uci.edu/dataset/366/activity+recognition+system+based+on+multisensor+data+fusion+arem) | 2016 | Activity Recognition system based on Multisensor data fusion (AReM) dataset | 7 | 1 | bending, cycling, lying down, sitting, standing, walking | Wireless Sensor Network (IRIS nodes) on chest, right ankle, left ankle |
| ⬜ |[SKODA](http://har-dataset.org/doku.php?id=wiki%3Adataset) | 2009 | - | - | 1 | 10: write notes, open engine hood, close engine hood, check door gaps, open door, close door, open/close two doors, check trunk gap, open/close trunk, check steering wheel. | 20 3D acceleration sensors (60 attributes) |

---

### Group 3: Multi-Modal Systems (IMU + Vision/Spatial)

*(Refined: Strictly combinations of wearable IMUs with non-time-series, high-dimensional modalities like RGB video, depth cameras, or optical motion capture).*

| Supported | Name | Year | Paper | Citations | Subjects | Activities | Sensors |
| --- | --- | --- | --- | --- | --- | --- | --- |
| ⬜ | [UTD-MHAD](https://personal.utdallas.edu/~kehtar/UTD-MHAD.html) | 2015 | *UTD-MHAD: A Multimodal Dataset for Human Action Recognition Utilizing a Depth Camera and a Wearable Inertial Sensor* | 997 | 8 subjects (4 females and 4 males) | 27 actions, including swipes, waves, claps, throws, drawing gestures, sports gestures, walking/jogging, and basic transitions like sit-to-stand and squats. | Microsoft Kinect sensor and a wearable inertial sensor worn on the right wrist or thigh depending on the action, consisting of a 9-axis MEMS sensor (3-axis acceleration, 3-axis angular velocity, 3-axis magnetic strength), sampled at 50 Hz |
| ⬜ | [UP-Fall](https://sites.google.com/up.edu.mx/har-up/) | 2019 | *UP-fall detection dataset: A multimodal approach* | 462 | 17 healthy young adults | 11 activities (6 daily living like walking, sitting, and jumping; 5 fall types like forward or backward falls) | 5 wearable sensors (accelerometer, gyroscope, light), 6 infrared sensors, EEG, and 2 cameras. |
| ⬜ | [SHL](http://www.shl-dataset.org/) | 2018 | *The University of Sussex-Huawei Locomotion and Transportation Dataset for Multimodal Analytics with Mobile Devices* | 317 | 3 | 8 locomotion/transportation modes — Car, Bus, Train, Subway, Walk, Run, Bike, Still | Multi-modal sensors from 4 smartphones (torso, backpack, hand, pocket), (accelerometer, gyroscope, likely magnetometer, GPS) and a body-worn camera |
| ⬜ | [Mmact](https://mmact19.github.io/2019/) | 2019 | *Mmact: A large-scale dataset for cross modal human action understanding* | 145 | 40 participants (20 female, 20 male) | 37 classes (daily activities, abnormal actions, desk work actions) | 7 modalities – accelerometer, gyroscope, orientation, RGB video, keypoints, Wi-Fi, pressure |
| ⬜ | [WEAR](https://mariusbock.github.io/wear/) | 2024 | *Wear: An outdoor sports dataset for wearable and egocentric activity recognition* | 66 | 22 | 8 different workout activities | synchronized inertial (acceleration) and camera (egocentric video) data |
| ⬜ | [LARa](https://zenodo.org/records/8189341) | 2020 | *Lara: Creating a dataset for human activity recognition in logistics using semantic attributes* | 119 | 16 | 8 (picking and packing scenarios in logistics) | IMUs (MbientLab and MotionMiners sensors), optical marker-based motion capture (OMoCap) system |
| ⬜ | [CIP1](https://github.com/ManuelPalermo/HumanInertialPose) | 2022 | *Complete Inertial Pose Dataset: from raw measurements to pose with low-cost and high-end MARG sensors* | 3 | 21 participants (15 males, 6 females) with an average age of 25.0 years | 6 types of movement sequences designed to capture a wide range of human dynamics | 9 MPU9250 sensors embedded in a smart garment prototype, sampling at approximately 100 Hz. These sensors were placed on the upper-body: head, upper/lower back, and both hands, forearms, and arms, Xsens Awinda Motion Capture system using the MVN BIOMECH biomechanical model |
| ⬜ | [CIP2](https://github.com/ManuelPalermo/HumanInertialPose) | 2022 | *Complete Inertial Pose Dataset: from raw measurements to pose with low-cost and high-end MARG sensors* | 3 | 10 participants (6 males, 4 females) with an average age of 24.2 years | 6 types of movement sequences designed to capture a wide range of human dynamics | 17 Xsens MTw Awinda sensors secured with straps, sampling at 60 Hz. Sensors were placed full-body on the head, chest, waist, and all main segments of the arms and legs, Xsens Awinda Motion Capture system using the MVN BIOMECH biomechanical model |


---

### Group 4: Unclear or not Publicy Available

| Supported | Name | Year | Paper | Citations | Subjects | Activities | Sensors |
| --- | --- | --- | --- | --- | --- | --- | --- |
| ⬜ | DA | 2012 | *Recognizing Human Activities User-independently on Smartphones Based on Accelerometer Data* | 302 | 8 | 5 everyday activities: walking, running, cycling, driving a car and sitting/standing | Nokia N8 smartphone in the trouser pocket, accelerometer at 40 Hz |
| ⬜ | [MobiAct](https://bmi.hmu.gr/the-mobifall-and-mobiact-datasets-2/#) | 2016 | *The MobiAct dataset: recognition of activities of daily living using smartphones* | 364 | 66 | 12 ADLs and 4 types of falls, plus a scenario of daily living; ADLs include fall-like activities (sitting, stepping in/out of a car), sudden/rapid movements (jumping, jogging), and common everyday movements (walking, standing, stairs up/down). | Smartphone (Samsung Galaxy S3) with 3-axis accelerometer, 3-axis gyroscope, and software-based orientation |
| ⬜ | [MobiFall](https://bmi.hmu.gr/the-mobifall-and-mobiact-datasets-2/#) | 2014 | *The MobiFall Dataset: Fall Detection and Classification with a Smartphone* | 128 | - | different types of falls and fall-like ADLs | Smartphone (Samsung Galaxy S3) with LSM330DLC inertial module, including 3-axis accelerometer, 3-axis gyroscope, and software-derived orientation (from accelerometer and geomagnetic field sensor). Data captured at the highest sampling rate |
| ⬜ | PAR | 2021 | *Context-aware support for cardiac health monitoring using federated machine learning* | 12 | 12 | 4 (sitting, standing, walking, jogging) | smartphone acc 50 Hz in pocket, holter monitor for ecg signals and heartbeat info |
| ⬜ | CHARM | 2021 | *A recommendation specific human activity recognition dataset with mobile device's sensor data* | 5 | 31 (aged 25–50) | 10 (sitting (chair/couch), standing, lying (up/side), walking, running, walking upstairs, walking downstairs, device on surface) | tri-axial smartphone accelerometer (20Hz), gyroscope |

---

### Group 4: No Subject Info -> No Loso possible

| Supported | Name | Year | Paper | Citations | Subjects | Activities | Sensors |
| --- | --- | --- | --- | --- | --- | --- | --- |
| ⬜ | [CHAD](https://www.utwente.nl/en/eemcs/ps/research/dataset/) | 2016 | *Complex human activity recognition using smartphone and wrist-worn motion sensors* | 554 | - | 13 activities—walk, stand, jog, sit, bike, upstairs, downstairs, type, write, coffee, talk, smoke, eat. | Smartphone and wrist-worn motion sensors recording accelerometer (x, y, z), linear acceleration (x, y, z), gyroscope (x, y, z), and magnetometer (x, y, z) |
| ⬜ | [PARDUSS](https://www.utwente.nl/en/eemcs/ps/research/dataset/) | 2013 | *Towards physical activity recognition using smartphone sensors* | 345 | - | dynamic, manually entered | Accelerometer, Gyroscope, Magnetometer, GPS, 50 Hz sampling |


---


| ⬜ | [UESTC-MMEA-CL](https://github.com/Xu-Linfeng/UESTC_MMEA_CL_main) | 2023 | *Towards Continual Egocentric Activity Recognition: A Multi-Modal Egocentric Activity Dataset for Continual Learning* | 57 | 10 | 32 daily activities, including basic movements, indoor behaviors, cleaning labor, and recreational activities | Self-developed smart glasses with an egocentric camera (640×480 at 25 FPS) and IMU (accelerometer and gyroscope) sampled at 25 Hz |
| ⬜ | [Berkeley MHAD]() | 2013 | *Berkeley MHAD: A Comprehensive Multimodal Human Action Database* | 668 | 12 subjects (7 males, 5 females) | 11 actions: jumping in place, jumping jacks, bending, punching, waving (one or two hands), clapping, throwing, sit down, stand up, and sit down/stand up | Optical motion capture system (Impulse, 480 Hz), 12 multi-view stereo cameras (Dragonfly2, 640x480, ~22 Hz), 2 Microsoft Kinect cameras (depth & color, 30 Hz), 6 wireless 3-axis accelerometers (30 Hz), 4 microphones (48 kHz) |
| ⬜ | [EPIC-KITCHENS-100]() | 2022 | *Rescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-KITCHENS-100* | 792 | 37 participants | ~90k fine-grained action segments capturing unscripted daily activities in 45 kitchens, involving 97 verb classes and 300 noun classes (hand-object interactions) | Head-mounted GoPro Hero7 Black (RGB video at 50fps, Audio) |
| ⬜ | [Stanford-ECM](http://vision.stanford.edu/projects/ego-energy-activity) | 2017 | *Jointly Learning Energy Expenditures and Activities Using Egocentric Multimodal Signals* | 100 | 10 participants | ~35 hours of data covering 24 diverse activities, including gym exercises, commuting, office work, and household chores | Head-mounted GoPro Hero 4 Session (video), chest-mounted iPhone 5s (accelerometer, gyroscope, audio), and wrist-worn Empatica E4 (heart rate, galvanic skin response, skin temperature) |
| ⬜ | [CMU-MMAC]() | 2009 | *Temporal Segmentation and Activity Classification from First-person Sensing* | 358 | 43 | Cooking and food preparation activities (5 recipes: brownies, pizza, sandwich, salad, scrambled eggs) | Wearable camera, microphones, and 5 wearable 3-axis IMUs (accelerometer, gyroscope, magnetometer) placed on the torso, arms, and legs |
| ⬜ | [MEAD]() | 2016 | *Multimodal Multi-stream Deep Learning for Egocentric Activity Recognition* | 107 | Multiple | 20 life-logging activities grouped into 4 categories: Ambulation, Daily Activities, Office Work, and Exercise (e.g., walking, eating, working at PC, running, push-ups) | Google Glass (synchronized video and sensor streams including accelerometer, gyroscope, magnetic field, rotation vector, gravity, and linear acceleration) |
| ⬜ | [EmoPain]() | 2024 | *The EmoPain@Home Dataset: Capturing Pain Level and Activity Recognition for People With Chronic Pain in Their Homes* | 10 | 18 participants (with and without chronic pain) | Routine daily activities in home settings (9 activity types) | Body movement data with continuous labels for pain, worry, and movement confidence |
| ⬜ | [MMC-PCL-Activity]() | 2023 | *Towards a multimodal human activity dataset for healthcare* | 15 | 14 graduate students | 16 daily activities (Level 2), including sleep, nap, eat, housework (mop/sweep), work/study (meeting, computer work, read), leisure (TV, games, music, chat), and exercise (run, walk) | Android smartphones (accelerometer & gyroscope at 100Hz, GPS, app usage, third-perspective images) and Mi Band 3 (heart rate, steps at ~0.05-0.1Hz) |
| ⬜ | [Ego4D]() | 2022 | *Ego4D: Around the World in 3,000 Hours of Egocentric Video* | 1763 | 931 participants | Unscripted daily life activities spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured in 74 worldwide locations | 7 different head-mounted cameras (e.g., GoPro, Vuzix Blade) capturing RGB video; subsets include audio, 3D scans (Matterport3D), gaze, stereo, and IMU data |